# -*- coding: utf-8 -*-
"""Handles interaction with the vector store (PostgreSQL + pgvector).

This module is responsible for connecting to the PostgreSQL database
(configured via environment variables) and inserting or updating text chunks,
their associated metadata, and their vector embeddings into a designated table
optimized for vector similarity search using the pgvector extension.

It's designed with asynchronous operations in mind (`async def`) to align with
modern Python web frameworks like FastAPI (used in `modular-dashboard`), although
the current implementation uses the synchronous `psycopg2` library as a placeholder.
A full async implementation would typically use `asyncpg` directly or via an
async ORM like SQLAlchemy's asyncio extension. Includes logging.
"""

import os
import sys
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional
# Database driver (standard synchronous PostgreSQL adapter)
import psycopg2
import psycopg2.extras # For potential helpers like DictCursor
# Import dotenv to load environment variables
from dotenv import load_dotenv, find_dotenv

# Get a logger instance for this module
logger = logging.getLogger(__name__)

# --- Database Configuration ---
def load_db_config() -> Dict[str, Optional[str]]:
    """Loads PostgreSQL connection parameters from environment variables.

    Searches for a `.env` file and loads it. Reads the following variables:
    - DB_HOST (default: 'localhost')
    - DB_PORT (default: '5432')
    - DB_NAME (mandatory)
    - DB_USER (mandatory)
    - DB_PASSWORD (mandatory)

    Logs warnings if mandatory variables are missing but does not raise an
    error here; the connection attempt will fail later.

    Returns:
        A dictionary containing the database connection parameters.
    """
    env_path = find_dotenv()
    if env_path:
        logger.info(f"Loading database config from: {env_path}")
        load_dotenv(dotenv_path=env_path, override=True)
    else:
        logger.warning(".env file not found for database config.")

    config = {
        "host": os.getenv("DB_HOST", "localhost"),
        "port": os.getenv("DB_PORT", "5432"),
        "database": os.getenv("DB_NAME"),
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
    }
    # Log missing essential variables as warnings
    if not config["database"]: logger.warning("DB_NAME not found in environment variables.")
    if not config["user"]: logger.warning("DB_USER not found in environment variables.")
    if not config["password"]: logger.warning("DB_PASSWORD not found in environment variables.")

    # Log loaded config (excluding password for security)
    logger.info(f"DB Config Loaded: Host={config['host']}, Port={config['port']}, "
                f"DB={config['database']}, User={config['user']}")
    return config

# --- Database Interaction ---
# Note: Marked async for future compatibility, but uses sync psycopg2 internally for now.
async def add_chunks_to_vector_store(rag_chunks: List[Dict[str, Any]]):
    """Adds or updates text chunks, metadata, and embeddings in PostgreSQL.

    Connects to the PostgreSQL database using credentials from the environment.
    Iterates through the provided list of RAG chunks. For each valid chunk
    (containing chunk_id, text_content, metadata, and embedding), it formats
    the data (metadata to JSON string, embedding list to vector string) and
    executes an `INSERT ... ON CONFLICT DO UPDATE` SQL query against the
    target table (defined internally, requires user configuration).

    Uses a single transaction; if any insertion fails, the entire transaction
    is rolled back.

    Args:
        rag_chunks: A list of dictionaries, where each dictionary represents a
                    chunk generated by the `formatter` and enriched by the
                    `embedding_generator`. Expected keys: 'chunk_id',
                    'text_content', 'metadata' (dict), 'embedding' (List[float]).

    Raises:
        ConnectionError: If database credentials are missing in the environment
                         or if the connection to the database fails.
        ValueError: If chunk data formatting fails (e.g., non-serializable metadata,
                    invalid embedding format). Currently logs warnings and skips.
        Exception: For database errors during the INSERT/UPDATE operation or
                   other unexpected issues. Logs errors before raising.
    """
    if not rag_chunks:
        logger.warning("Vector Store Handler: No chunks provided to add.")
        return

    logger.info(f"--- Adding {len(rag_chunks)} Chunks to Vector Store (PostgreSQL) ---")
    db_config = load_db_config()
    conn = None
    inserted_count = 0
    skipped_count = 0

    # --- Pre-connection Check ---
    if not all([db_config["database"], db_config["user"], db_config["password"]]):
         error_msg = "Database connection details missing in .env (DB_NAME, DB_USER, DB_PASSWORD). Cannot connect."
         logger.critical(error_msg)
         raise ConnectionError(error_msg)

    try:
        # --- Connect (Sync Example) ---
        # TODO: Replace with async connection (e.g., asyncpg.create_pool or SQLAlchemy async engine)
        logger.info(f"Connecting to PostgreSQL database '{db_config['database']}' on {db_config['host']}...")
        conn = psycopg2.connect(**db_config)
        conn.autocommit = False # Explicit transaction management
        cur = conn.cursor()
        logger.info("Database connection successful.")

        # --- Prepare SQL Query ---
        # !!! IMPORTANT: User must configure these table/column names !!!
        table_name = os.getenv("DB_VECTOR_TABLE", "your_vector_table") # Example: Allow override via env var
        chunk_id_col = "chunk_id"
        text_col = "text_content"
        metadata_col = "metadata" # Should be JSONB type in Postgres
        vector_col = "embedding_vector" # Should be VECTOR(dimension) type

        # Upsert query: Inserts if chunk_id is new, updates if it exists.
        insert_query = f"""
            INSERT INTO {table_name} ({chunk_id_col}, {text_col}, {metadata_col}, {vector_col})
            VALUES (%s, %s, %s, %s)
            ON CONFLICT ({chunk_id_col}) DO UPDATE SET
                {text_col} = EXCLUDED.{text_col},
                {metadata_col} = EXCLUDED.{metadata_col},
                {vector_col} = EXCLUDED.{vector_col};
        """
        logger.info(f"Preparing to insert/update data into table '{table_name}'...")

        # --- Iterate and Insert/Update Chunks ---
        for chunk in rag_chunks:
            chunk_id = chunk.get("chunk_id")
            text_content = chunk.get("text_content")
            metadata = chunk.get("metadata")
            embedding = chunk.get("embedding")

            # Validate required data for insertion
            if not chunk_id or not text_content or metadata is None or embedding is None:
                logger.warning(f"Skipping chunk ID '{chunk_id}' due to missing required data (id, text, meta, or embedding).")
                skipped_count += 1
                continue

            # --- Format Data for SQL ---
            try:
                # 1. Metadata to JSON string
                metadata_json = json.dumps(metadata)
                # 2. Embedding list to pgvector string format '[f1,f2,...]'
                if isinstance(embedding, list) and all(isinstance(x, (int, float)) for x in embedding):
                    # Basic string conversion, might need escaping if content has brackets/commas
                    embedding_str = str(embedding).replace(" ", "")
                else:
                    logger.warning(f"Skipping chunk ID '{chunk_id}': Invalid embedding format (not a list of numbers).")
                    skipped_count += 1
                    continue
            except TypeError as e:
                logger.warning(f"Skipping chunk ID '{chunk_id}' due to non-serializable metadata: {e}")
                skipped_count += 1
                continue
            except Exception as fmt_e:
                 logger.warning(f"Skipping chunk ID '{chunk_id}' due to data formatting error: {fmt_e}", exc_info=True)
                 skipped_count += 1
                 continue

            # --- Execute Query ---
            try:
                logger.debug(f"Executing upsert for chunk ID: {chunk_id}")
                # Pass parameters tuple to cursor.execute for safe insertion
                cur.execute(insert_query, (chunk_id, text_content, metadata_json, embedding_str))
                inserted_count += 1
            except psycopg2.Error as insert_error:
                 # Log database-specific errors
                 logger.error(f"Database error inserting/updating chunk ID {chunk_id}: {insert_error}", exc_info=True)
                 conn.rollback() # Rollback transaction on first error
                 # Re-raise to stop processing further chunks in this batch
                 raise Exception(f"Database error during upsert for chunk {chunk_id}") from insert_error
            except Exception as exec_error:
                 # Catch other potential errors during execution
                 logger.error(f"Unexpected error executing query for chunk ID {chunk_id}: {exec_error}", exc_info=True)
                 conn.rollback()
                 raise # Re-raise

        # --- Commit Transaction ---
        # If the loop completes without raising an exception, commit all changes
        conn.commit()
        logger.info(f"Transaction committed. Successfully added/updated {inserted_count} chunks.")
        if skipped_count > 0:
             logger.warning(f"Skipped {skipped_count} chunks due to missing data or formatting errors.")

    except psycopg2.Error as db_error:
        # Catch connection errors or errors outside the loop/commit
        logger.critical(f"Database connection or setup error: {db_error}", exc_info=True)
        if conn: conn.rollback() # Ensure rollback if connection was partially successful
        # Wrap in ConnectionError for clarity upstream
        raise ConnectionError(f"Database connection or operation failed: {db_error}") from db_error
    except Exception as e:
        # Catch any other unexpected errors in the main try block
        logger.critical(f"An unexpected error occurred interacting with the vector store: {e}", exc_info=True)
        if conn: conn.rollback()
        raise # Re-raise other errors
    finally:
        # --- Close Connection ---
        # Ensure the connection is always closed if it was opened
        if conn:
            conn.close()
            logger.info("Database connection closed.")


# Example usage block (for testing when script is run directly)
if __name__ == "__main__":
    # Configure logging for test run
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
    logger.info("--- Running vector_store_handler.py directly for testing ---")

    # --- !!! DANGER ZONE !!! ---
    logger.warning("This test block WILL attempt to connect to and write to the database.")
    logger.warning("Ensure you are using a TEST database and have created the table.")
    logger.warning("Verify table/column names and vector dimension in the script and .env.")

    # Sample chunks with embeddings
    embedding_dim = 1536 # Match your table definition (e.g., text-embedding-3-small)
    sample_rag_chunks_with_embeddings = [
        {"chunk_id": "test_chunk_001_v2", "text_content": "Updated test chunk 1.", "metadata": {"s": "t1-upd"}, "embedding": [0.15] * embedding_dim},
        {"chunk_id": "test_chunk_002_v2", "text_content": "Updated test chunk 2.", "metadata": {"s": "t2-upd"}, "embedding": [0.25] * embedding_dim},
        {"chunk_id": "test_chunk_003_new", "text_content": "A new test chunk.", "metadata": {"s": "t3-new"}, "embedding": [0.3] * embedding_dim}
    ]
    logger.info(f"Sample Chunks to Add/Update: {len(sample_rag_chunks_with_embeddings)}")

    confirm = input("\nProceed with test database insertion/update? (yes/no): ")

    if confirm.lower() == 'yes':
        logger.info("Proceeding with test database operation...")
        try:
            # Use asyncio.run() because the target function is async
            asyncio.run(add_chunks_to_vector_store(sample_rag_chunks_with_embeddings))
            logger.info("Test database operation process completed (check database).")
        except ConnectionError as e:
            logger.error(f"Test failed due to connection error: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during testing: {e}", exc_info=True)
    else:
        logger.info("Test database operation cancelled by user.")

    logger.info("--- Vector Store Handler Test Complete ---")